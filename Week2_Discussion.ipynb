{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trprince21/STATS101C_notes/blob/main/Week2_Discussion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Cross Etropy (BCE)\n",
        "$= -(y \\log(q) + (1-y) \\log(1-q))$\n",
        "\n",
        "$E[Y] = p$\n",
        "$E_p[BCE] = -(-p \\log q + (1-p) \\log (1-q))$\n",
        "\n",
        "True: $P(y=1) = p$\n",
        "      $P(y=0) 1-p$\n",
        "\n",
        "Predicted: $Q(y=1) = q$\n",
        "           $Q(y=0) = 1-q$\n",
        "\n",
        "**[REFER TO PHOTOS FOR EXAMPLE W/ A GRAPH]**\n",
        "\n",
        "\n",
        "KL-Divergence -> Distance Measure\n",
        "                 btw 2 distributions\n",
        "\n",
        "$D_{KL} = p \\log(\\frac{p}{q}) + (1-p) \\log(\\frac{1-p}{1-q})\n",
        "        = p(\\log p - \\log q) + (1-p) (\\log (1-p) - \\log (1-q))\n",
        "        = p \\log p + (1-p) \\log (1-p) - p \\log q - (1-p) \\log (1-q)$\n",
        "      ((=       entropy ((p?))        -       minimize             ))\n",
        "\n",
        "        MLE\n",
        "       -log likelihood\n",
        "        Binomial(n, q) -> l\n",
        "\n",
        "**[REFER TO PHOTOS FOR GRAPH]**\n",
        "\n",
        "$L = \\prod_{i=1}^n q^{y_i} (1-q)^{1-y_i}$\n",
        "$-l = -[y_i \\log q + (1-y_i) \\log (1-q)]$\n",
        "\n",
        "\n",
        "Gradient Descent (for Linear Regression?)\n",
        "\n",
        "Cost function: MSE  1) Normal Dist. of errors\n",
        "                    2) Simplicity and Differentiability\n",
        "\n",
        "Model: $y = mx + b + \\epsilon$   $\\epsilon$ ~ $\\mathcal{N}(0, \\sigma^2)$\n",
        "$J(m,b) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\sum_{i=1}^n (m x_i + b - y_i)^2$\n",
        "$\\frac{\\partial J(m,b)}{\\partial m} = \\frac{1}{n} \\sum x_i(m x_i + b - y_i)$\n",
        "$\\frac{\\partial J(m,b)}{\\partial b} = \\frac{1}{n} \\sum (m x_i + b - y_i)$"
      ],
      "metadata": {
        "id": "IUjLvRO3pCgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do some gradient descent coding questions:\n",
        "\n",
        "First, let's do $f(x) = x^2 + 3x + 2$\n",
        "\n",
        "Define:\n",
        "- objective function\n",
        "- a function for gradient descent\n",
        "- print out the value of x in every iteration\n",
        "\n",
        "Initial parameters\n",
        "- learning rate\n",
        "- initial start point (x)\n",
        "- Number of iterations\n",
        "\n",
        "def obj_func(x):\n",
        "  return x**2 + 3*x + 2\n",
        "\n",
        "def gradient_descent(learning_rate, initial_x, iterations):\n",
        "  x = initial_x\n",
        "  for i in range(iterations):\n",
        "    x = x - learning_rate*2*x+3\n",
        "  \n",
        "  return(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xmlLWIa1h6vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "initial_x = 0\n",
        "iterations = 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Objective function: f(x) = x^2 + 3x + 2\n",
        "def objective_function(x):\n",
        "    return x**2 + 3*x + 2\n",
        "\n",
        "\n",
        "def gradient_descent(learning_rate, initial_x, iterations):\n",
        "    x = initial_x\n",
        "    for i in range(iterations):\n",
        "        x = x - learning_rate * (2*x + 3)\n",
        "        print(\"Iteration \", i+1, \": x = \", x, \"at f(x) = \", objective_function(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "final_x = gradient_descent(learning_rate, initial_x, iterations)\n",
        "print(\"Final optimized x:\", final_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdwNK01ch6kq",
        "outputId": "75443253-18a8-4c5a-e26c-430082771206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1 : x =  -0.30000000000000004 at f(x) =  1.19\n",
            "Iteration  2 : x =  -0.54 at f(x) =  0.6716\n",
            "Iteration  3 : x =  -0.732 at f(x) =  0.3398240000000001\n",
            "Iteration  4 : x =  -0.8855999999999999 at f(x) =  0.12748736000000038\n",
            "Iteration  5 : x =  -1.00848 at f(x) =  -0.008408089600000057\n",
            "Iteration  6 : x =  -1.106784 at f(x) =  -0.09538117734399965\n",
            "Iteration  7 : x =  -1.1854272 at f(x) =  -0.1510439535001602\n",
            "Iteration  8 : x =  -1.24834176 at f(x) =  -0.18666813024010231\n",
            "Iteration  9 : x =  -1.298673408 at f(x) =  -0.20946760335366577\n",
            "Iteration  10 : x =  -1.3389387264 at f(x) =  -0.22405926614634541\n",
            "Final optimized x: -1.3389387264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can move on to a 3D function: $f(x, y) = x^2 + y^2 + 3xy + 2x + 2y + 5$\n",
        "\n",
        "Calculate partial derivatives wrt x and then wrt y\n",
        "\n",
        "Gradient of our obj_function\n",
        "\n",
        "for x: 2x + 3y + 2\n",
        "for y: 2y + 3x + 2\n"
      ],
      "metadata": {
        "id": "Av62HnSEh5_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.1\n",
        "initial_point = [0, 0]\n",
        "iterations = 10\n",
        "\n",
        "\n",
        "\n",
        "def objective_function(x, y):\n",
        "    return x**2 + y**2 + 3*x*y + 2*x + 2*y + 5\n",
        "\n",
        "\n",
        "def gradient_descent(learning_rate, initial_point, iterations):\n",
        "    point = np.array(initial_point)\n",
        "    for i in range(iterations):\n",
        "        grad = np.array([2*point[0] + 3*point[1] + 2, 2*point[1] + 3*point[0] + 2])\n",
        "        point = point - learning_rate * grad\n",
        "        print(\"Iteration \", i+1, \": x = \", point[0], \"y = \", point[1], \"at f(x) = \", objective_function(point[0],point[1]))\n",
        "    return point\n",
        "\n",
        "final_point = gradient_descent(learning_rate, initial_point, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9joCV0h5kn",
        "outputId": "8c0cb40c-f8f1-4090-bdcc-7739be7d8e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1 : x =  -0.2 y =  -0.2 at f(x) =  4.4\n",
            "Iteration  2 : x =  -0.30000000000000004 y =  -0.30000000000000004 at f(x) =  4.25\n",
            "Iteration  3 : x =  -0.35000000000000003 y =  -0.35000000000000003 at f(x) =  4.2125\n",
            "Iteration  4 : x =  -0.37500000000000006 y =  -0.37500000000000006 at f(x) =  4.203125\n",
            "Iteration  5 : x =  -0.3875 y =  -0.3875 at f(x) =  4.20078125\n",
            "Iteration  6 : x =  -0.39375 y =  -0.39375 at f(x) =  4.2001953125\n",
            "Iteration  7 : x =  -0.396875 y =  -0.396875 at f(x) =  4.200048828125\n",
            "Iteration  8 : x =  -0.3984375 y =  -0.3984375 at f(x) =  4.20001220703125\n",
            "Iteration  9 : x =  -0.39921875 y =  -0.39921875 at f(x) =  4.200003051757813\n",
            "Iteration  10 : x =  -0.399609375 y =  -0.399609375 at f(x) =  4.200000762939453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent for Linear regression"
      ],
      "metadata": {
        "id": "NSrwPAXYP2sV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5AjIJ6dPh46"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.normal(0,5, size = 1000)\n",
        "slope = 4\n",
        "intercept = 2\n",
        "y = slope*x + intercept + np.random.normal(0,1,size = 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudocode: \\\\\n",
        "- Obtain gradient equations (Our cost function is now $J(m,b) = \\frac{1}{2n}\\sum_{i = 1}^n (\\hat{y} - y)^2$)\n",
        "- Make predictions of $\\hat{y}$\n",
        "- Update slope and intercept\n",
        "- repeat\n",
        "\n",
        "\n",
        "Initializations:\n",
        "- slope\n",
        "- intercept\n",
        "- learning rate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WsV_DONrP1jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "initial_slope = 0.5\n",
        "initial_intercept = 0.5\n",
        "iterations = 1000\n",
        "\n",
        "\n",
        "def objective_function(x, y, slope, intercept):\n",
        "  return np.mean((y - (slope*x + intercept))**2)\n",
        "\n",
        "\n",
        "\n",
        "def gradient_descent(lr, initial_slope, initial_intercept):\n",
        "  slope = initial_slope\n",
        "  intercept = initial_intercept\n",
        "  for i in range(iterations):\n",
        "    y_pred = slope*x + intercept\n",
        "    grad_slope = 2*np.mean(x*(y_pred - y))\n",
        "    grad_intercept = 2*np.mean(y_pred - y)\n",
        "    slope = slope  - lr*grad_slope\n",
        "    intercept = intercept - lr*grad_intercept\n",
        "    if i % 100 == 0:\n",
        "      print(\"Iteration \", i+1, \": slope = \", slope, \"intercept = \", intercept, \"at f(x) = \", objective_function(x, y, slope_update, intercept_update))\n",
        "  return slope, intercept\n",
        "\n",
        "slope, intercept = gradient_descent(lr, slope, intercept)"
      ],
      "metadata": {
        "id": "WMszTgN1P2AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZNneT4HL19L3"
      }
    }
  ]
}